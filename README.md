# ğŸ§  AI Wellness Recommendation API

API backend desenvolvida com **FastAPI + PostgreSQL + Ollama (LLM
local)** para geraÃ§Ã£o inteligente de recomendaÃ§Ãµes de bem-estar
personalizadas com base no perfil do usuÃ¡rio.

Projeto desenvolvido como parte de teste tÃ©cnico para vaga de
Desenvolvedor Backend Python com foco em IA.

------------------------------------------------------------------------

# ğŸš€ Tecnologias Utilizadas

-   Python 3.11+
-   FastAPI
-   PostgreSQL
-   SQLAlchemy (ORM)
-   SQL Raw (queries analÃ­ticas)
-   Pydantic
-   Poetry (gerenciamento de dependÃªncias)
-   Ollama (LLM local -- Llama3)
-   Pytest
-   Pytest-asyncio

------------------------------------------------------------------------

# ğŸ— Arquitetura do Projeto

app/
â”œâ”€â”€ main.py
â”œâ”€â”€ database.py
â”œâ”€â”€ core/
â”œâ”€â”€ models/
â”œâ”€â”€ schemas/
â”œâ”€â”€ crud/
â”œâ”€â”€ routers/
â”œâ”€â”€ service/
â””â”€â”€ utils/

tests/
â”œâ”€â”€ test_users.py

Arquitetura modular seguindo boas prÃ¡ticas de backend.

------------------------------------------------------------------------

# ğŸ§  IntegraÃ§Ã£o com IA

Fluxo:

1.  UsuÃ¡rio envia contexto
2.  Sistema consulta perfil no banco
3.  Prompt estruturado Ã© construÃ­do
4.  LLM gera resposta em formato JSON
5.  Resposta Ã© validada via Pydantic
6.  Resultado Ã© persistido no banco

Formato esperado da LLM:

{ "activities": \[ { "name": "...", "description": "...", "duration":
"...", "category": "..." } \], "reasoning": "...", "precautions": "..."
}

------------------------------------------------------------------------

# ğŸ›  Como Rodar o Projeto

## 1ï¸âƒ£ Clonar o repositÃ³rio

git clone https://github.com/gMoraes1/namu-ai cd namu-ai

## 2ï¸âƒ£ Instalar dependÃªncias com Poetry

pip install poetry poetry install poetry shell

## 3ï¸âƒ£ Configurar variÃ¡veis de ambiente

Criar arquivo .env:

DB_HOST=localhost
DB_PORT=5432
DB_USER=namu
DB_PASSWORD=namu123
DB_NAME=namu_ai

# LLM Configuration 
LLM_MODEL=llama3
OLLAMA_BASE_URL=http://localhost:11434

## 4ï¸âƒ£ Rodar PostgreSQL via Docker Compose

docker compose up -d

## 5ï¸âƒ£ Instalar e rodar ollama

Linux : curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3 ollama run llama3

## 6ï¸âƒ£ Rodar API

uvicorn app.main:app --reload

DocumentaÃ§Ã£o automÃ¡tica: http://localhost:8000/docs

------------------------------------------------------------------------

# ğŸ“Œ Endpoints

## UsuÃ¡rios

POST /users\
GET /users/{user_id}\
GET /users/{user_id}/recommendations\
GET /users/{user_id}/stats

## RecomendaÃ§Ãµes

POST /recommendations\
POST /recommendations/{recommendation_id}/feedback

------------------------------------------------------------------------

# ğŸ”’ Tratamento de Erros

-   404 para recursos inexistentes
-   500 para falha no parsing da LLM
-   ValidaÃ§Ã£o via Pydantic
-   SQL raw com parÃ¢metros nomeados (proteÃ§Ã£o contra SQL Injection)

------------------------------------------------------------------------

# ğŸ“Š Diferenciais

âœ” IntegraÃ§Ã£o real com LLM local\
âœ” Prompt estruturado\
âœ” JSONB no PostgreSQL\
âœ” SQL raw com JOIN e agregaÃ§Ãµes\
âœ” Arquitetura modular\
âœ” Swagger automÃ¡tico
âœ” Testes automatizados com Pytest

------------------------------------------------------------------------

ğŸ”® PrÃ³ximas Melhorias
- ImplementaÃ§Ã£o completa de testes automatizados para todas as rotas
- SeparaÃ§Ã£o de banco de dados especÃ­fico para ambiente de testes
- ContainerizaÃ§Ã£o completa da aplicaÃ§Ã£o (API + DB + LLM)

------------------------------------------------------------------------

# ğŸ‘¨â€ğŸ’» Autor

Gustavo Moraes
